# -*- coding: utf-8 -*-
"""Credit_Risk_Modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q0PavkbV7bSLB16DU6j0hz6XxnQNCK6d
"""

!pip install pandas

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option("display.max_columns", None)
sns.set_style("whitegrid")

df = pd.read_csv("/content/german_credit_data.csv")

df.head()

df["Age"].describe()

df['Risk'].value_counts()

df.shape



df.info()

df.describe(include="all")

df["Job"].unique()

df.isna().sum()

df = df.dropna().reset_index(drop=True)

df

df.columns

df.drop(columns='Unnamed: 0', inplace = True)

df.columns

df[["Age", "Credit amount", "Duration"]].hist(bins = 5)
plt.suptitle("Distribution of Numerical Feature", fontsize = 14)
plt.show()

plt.figure(figsize=(15,5))
for i , col in enumerate(["Age", "Credit amount", "Duration"]):
  plt.subplot(1,3, i + 1)
  sns.boxplot(y = df[col], color="Skyblue")
  plt.title(col)
plt.tight_layout()
plt.show()

df.query("Duration >= 60")

categorical_cols = ["Sex","Job","Housing","Saving accounts","Checking account","Purpose"]

plt.figure(figsize=(15,10))

for i, col in enumerate(categorical_cols):
    plt.subplot(3, 3, i + 1)
    sns.countplot(
        data=df,
        x=col,
        hue=col,
        palette="Set2",
        order=df[col].value_counts().index,
        legend=False
    )
    plt.title(f"Distribution of {col}")
    plt.xticks(rotation=45)

plt.tight_layout()
plt.show()

corr = df[["Age","Job","Credit amount","Duration"]].corr()

corr

sns.heatmap(corr, annot= True,cmap ="coolwarm",fmt=".2f")
plt.show()

df.groupby("Job")["Credit amount"].mean()

df.groupby("Sex")["Credit amount"].mean()

pd.pivot_table(df, values="Credit amount", index = "Housing", columns= "Purpose")

sns.scatterplot(data=df, x="Age", y="Credit amount", hue = "Sex", size="Duration", alpha=0.7, palette="Set1")
plt.title("Credit amount vs Age colored by Sex and Sized by Duration")
plt.show()

sns.violinplot(data=df, x = "Saving accounts", y = "Credit amount", palette="Pastel1")
plt.title("Credit Amount Distribution by Savings Accounts")
plt.show()

df["Risk"].value_counts(normalize=True) * 100

plt.figure(figsize=(15,5))

for i, col in enumerate(["Age", "Credit amount", "Duration"]):
    plt.subplot(1, 3, i + 1)
    sns.boxplot(
        data=df,
        x="Risk",
        y=col,
        hue="Risk",
        palette="Pastel2",
        legend=False
    )
    plt.title(f"Risk vs {col}")

plt.tight_layout()
plt.show()

df.groupby("Risk")[["Age","Credit amount","Duration"]].mean()

plt.figure(figsize=(10,10))
for i, col in enumerate(categorical_cols):
  plt.subplot(3,3,i + 1)
  sns.countplot(data = df, x = col, hue = "Risk", palette="Set1", order=df[col].value_counts().index)
  plt.title(f"{col} by Risk")
  plt.xticks(rotation = 45)

plt.tight_layout()
plt.show()

feature = ["Age", "Sex", "Job", "Housing", "Saving accounts", "Checking account", "Credit amount", "Duration"]

target = "Risk"

df_model = df[feature + [target]].copy()

df_model.head()

from sklearn.preprocessing import LabelEncoder
import joblib

cat_cols = df_model.select_dtypes(include="object").columns.drop("Risk")

le_dict = {}

cat_cols

for col in cat_cols:
  le = LabelEncoder()
  df_model[col] = le.fit_transform(df_model[col])
  le_dict[col] = le
  joblib.dump(le, f"{col}_encoder.pkl")

le_target = LabelEncoder()

df_model[target]

df_model[target] = le_target.fit_transform(df_model[target])

df_model[target].value_counts()

joblib.dump(le_target, "target_encoder.pkl")

df_model.head()

from sklearn.model_selection import train_test_split

x = df_model.drop(target, axis = 1)

y = df_model[target]

x

y

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, stratify=y, random_state=1
)

x_train.shape

x_test.shape

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score

def train_model(model, param_grid, x_train, y_train, x_test, y_test):
    grid = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        cv=5,
        scoring="accuracy",
        n_jobs=-1
    )
    grid.fit(x_train, y_train)
    best_model = grid.best_estimator_
    y_pred = best_model.predict(x_test)
    acc = accuracy_score(y_test, y_pred)
    return best_model, acc, grid.best_params_

from sklearn.tree import DecisionTreeClassifier

dt = DecisionTreeClassifier()

dt_param_grid = {
    "criterion": ["gini", "entropy"],
    "max_depth": [3, 5, 7, 10, None],
    "min_samples_split": [2, 5, 10]
}

best_dt, acc_dt, params_dt = train_model(dt, dt_param_grid, x_train, y_train, x_test, y_test)

print("Decision Tree Accuracy", acc_dt)

print("Best Parameters", params_dt)

rf = RandomForestClassifier(random_state = 1, class_weight="balanced", n_jobs= -1)

rf_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

best_rf, acc_rf, params_rf = train_model(rf, rf_param_grid, x_train, y_train, x_test, y_test)

print("Random Forest Accuracy", acc_rf)

print("Best Parameters", best_rf)

et = ExtraTreesClassifier(random_state=1, class_weight="balanced", n_jobs= -1)

et_param_grid = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

best_et, acc_et, param_et = train_model(et, et_param_grid,  x_train, y_train, x_test, y_test)

print("Extra trees accuracy", acc_et)

print("Best Paramaters: ", param_et)

from scipy.special import eval_hermite
xgb = XGBClassifier(random_state = 1, scale_pos_weightc = (y_train ==0).sum() / (y_train ==1).sum(), use_label_encoder=False, eval_metric = "logloss")

xgb_param_grid = {
    "n_estimators" : [100,200],
    "max_depth" : [3,5,7],
    "learning_rate" : [0.01, 0.1, 0.2],
    "subsample" : [0.7, 1],
    "colsample_bytree" : [0.7,1]
}

best_xgb, acc_xgb, params_xgb = train_model(xgb, xgb_param_grid, x_train, y_train, x_test, y_test)

print("xgb accuracy", acc_xgb)

print("Best Paramaters", params_xgb)

best_et.predict(x_test)

joblib.dump(best_et, "extra_trees_credit_model.pkl")

!pip install streamlit

!streamlit run app.py

!pip install streamlit pyngrok

!pip install streamlit pyngrok joblib pandas

ngrok.kill()

!streamlit run app.py &>/dev/null&

from pyngrok import ngrok
!ngrok config add-authtoken 35CfXFgVNyWE6WJYhGI7Casnl9R_CCTN44P8uTVwe3cN2HqA

!streamlit run app.py &>/dev/null&
public_url = ngrok.connect(addr="8501")
print("Access your app here:", public_url.public_url)

